# -*- coding: utf-8 -*-
"""Clustering_Countries_by_Pritam.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GZGPgLb11wTru5v87GkwqUziO8dmQu9C
"""

# Importing the required libraries
# Pandas and Numpy libraries will be used for data manipulations, Matplotlib and Seaborn will be used for data visualizations which will be required for Analysis.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings

warnings.filterwarnings('ignore')

df = pd.read_csv("/content/Country-data.csv", index_col='country')
df.head()

df.shape      # the dataset has 9 features with 167 rows of data

df.info()    # Let's look at the datatypes of the features. Other than the Country feature which is Categorical rest all are of Numeric datatype

df.isna().sum()                     # There are no missing\null values present in the dataset

df.duplicated().sum()           # There are no duplicate values present in the dataset

# Let's do some pre analysis of the numeric data
df.describe()

"""### **Insights**
**Child Mortality (child_mort):**
Ranges from 2.6 to 208, with a high variability (std: 40.33), indicating significant disparities in child mortality across countries.

**Exports and Imports (exports, imports in % of GDP):**
Wide range for both metrics (Exports: 0.1–200, Imports: 0.07–174).
Median values are lower than the mean, suggesting some countries have disproportionately high trade percentages.

**Health Spending (health in % of GDP):**
Varies between 1.81% and 17.9%, with a mean of 6.82%. Most countries spend a modest proportion of GDP on health.

**Income and GDP (income, gdpp in $):**
High disparity in wealth (Income: 609–125,000; GDP: 231–105,000) with large standard deviations.
Median values are significantly lower than the mean, indicating economic inequality.

**Inflation (inflation):**
Highly variable, ranging from -4.21% to 104%, with extreme values suggesting economic instability in some regions.

**Life Expectancy (life_expec in years):**
Generally high (32.1–82.8) with a mean of 70.56, but the low minimum indicates poor health outcomes in some nations.

**Fertility Rate (total_fer):**
Ranges from 1.15 to 7.49, with a mean of 2.95, suggesting variations in population growth dynamics.

### **Univariate Analysis**
"""

df.columns

# Plot histograms for all the variables
plt.figure(figsize=(15, 10))
for i, column in enumerate(df.columns, 1):
    plt.subplot(3, 3, i)
    sns.histplot(df[column], kde=True, bins=30, color='blue', alpha=0.7)
    plt.title(f'Distribution of {column}')
    plt.xlabel(column)
    plt.ylabel('Frequency')
plt.tight_layout()
plt.show()

# Boxplots for detecting outliers
plt.figure(figsize=(15, 10))
for i, column in enumerate(df.columns, 1):
    plt.subplot(3, 3, i)
    sns.boxplot(x=df[column], color='orange')
    plt.title(f'Boxplot of {column}')
    plt.xlabel(column)
plt.tight_layout()
plt.show()

"""**Child Mortality:** Most countries have low child mortality, but a few exhibit extremely high values, indicating outliers.

**Life Expectancy:** Majority of countries have life expectancy above 65 years, with a few exceptions.

**Income and GDP per Capita:** Both show heavy right-skewness, with most countries clustered at lower values.

**Inflation:** A significant spread is observed, with a mix of deflation and hyperinflation cases.

**Fertility Rate:** Most countries have fertility rates below 4, aligning with modern demographic trends.

### **Bivariate and Multivariate Analysis**
"""

# Bivariate Analysis: Correlation Matrix and Scatter Plots

# Visualize the correlation matrix using a heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title("Correlation Matrix of Socio-Economic and Health Factors")
plt.show()


fig, axs = plt.subplots(2, 2, figsize=(15, 10))

# Relationship between Health Spending and Life Expectancy
sns.scatterplot(data=df, x='health', y='life_expec', ax=axs[0, 0])
axs[0, 0].set_title("Health Spending vs Life Expectancy")

# Relationship between Income and Child Mortality
sns.scatterplot(data=df, x='income', y='child_mort', ax=axs[0, 1])
axs[0, 1].set_title("Income vs Child Mortality")

# Relationship between Fertility Rate and Life Expectancy
sns.scatterplot(data=df, x='total_fer', y='life_expec', ax=axs[1, 0])
axs[1, 0].set_title("Fertility Rate vs Life Expectancy")

# Relationship between GDP per capita and Exports
sns.scatterplot(data=df, x='gdpp', y='exports', ax=axs[1, 1])
axs[1, 1].set_title("GDP per Capita vs Exports")

plt.tight_layout()
plt.show()

"""The **heatmap** shows the relationships between numerical variables in the dataset.

Key Findings:
Positive correlations (e.g. income and GDP per capita).
Negative correlations (e.g. child mortality and life expectancy).

**Scatter Plots:**
**Health Spending vs. Life Expectancy:**
A positive relationship suggests that higher spending on health is associated with longer life expectancy.

**Income vs. Child Mortality:**
A negative relationship indicates that higher incomes are associated with lower child mortality rates.

**Fertility Rate vs. Life Expectancy:**
A negative relationship highlights that higher fertility rates may correlate with lower life expectancy.

**GDP per Capita vs. Exports:**
A moderate positive relationship shows that countries with higher GDP per capita tend to have higher export percentages.
"""

df.corr()

"""**Multicollinearity check:**

We'll consider the features having a positive or negative correlation value of greater than 0.8 or less than -0.8 to have collinearity.

**Child Mortality (child_mort) and Life Expectancy (life_expec)**
Correlation: -0.89
Interpretation: As child_mort increases, life_expec tends to decrease significantly. These two variables capture overlapping aspects of development and health, so retaining both might lead to redundancy.

**Child Mortality (child_mort) and Total Fertility (total_fert)**
Correlation: 0.85
Interpretation: Higher child mortality tends to be associated with higher fertility rates. This relationship may reflect socio-economic and healthcare-related patterns.

**Income (income) and GDP per capita (gdpp)**
Correlation: 0.90
Interpretation: Higher income levels are strongly associated with higher GDP per capita. These variables are likely measuring similar aspects of economic prosperity.

"""

from statsmodels.stats.outliers_influence import variance_inflation_factor
import statsmodels.api as sm

# Function to calculate VIF
def calculate_vif(df, numerical_features):
    vif_data = pd.DataFrame()
    vif_data["Feature"] = numerical_features
    vif_data["VIF"] = [
        variance_inflation_factor(df[numerical_features].values, i) for i in range(len(numerical_features))
    ]
    return vif_data

# Calculate VIF for all numerical features
vif_result = calculate_vif(df, df.columns)
vif_result.sort_values(by="VIF", ascending=False)

plt.figure(figsize=(6,4))
sns.scatterplot(data=df, x='gdpp', y='life_expec', hue='child_mort', size='child_mort', sizes=(20, 200), palette='viridis')
plt.title("GDP Per Capita vs Life Expectancy with Child Mortality")
plt.xlabel("GDP Per Capita (gdpp)")
plt.ylabel("Life Expectancy (life_expec)")
plt.show()

"""From the above plot we can observe that the countries having low GDP per capita tend to have high child mortality rates provided the life expectancy in such countries is also low.

This insight provides a strong basis for targeting aid. Countries with low GDP per capita, high child mortality, and low life expectancy could be prioritized for resource allocation, as they exhibit the most pressing developmental needs.
"""

df.head()

outlier_summary = {}

# Function to calculate IQR and identify outliers
def iqr_outliers(data, feature):
    Q1 = data[feature].quantile(0.25)
    Q3 = data[feature].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = data[(data[feature] < lower_bound) | (data[feature] > upper_bound)]
    return lower_bound, upper_bound, outliers

# Apply the IQR method to detect outliers for each feature
for feature in df.columns:
    lower, upper, outliers = iqr_outliers(df, feature)
    total_count = df.shape[0]
    outlier_percentage = (outliers.shape[0] / total_count) * 100  # Calculate outlier percentage
    outlier_summary[feature] = {
        "Lower Bound": round(lower, 3),
        "Upper Bound": round(upper, 3),
        "Number of Outliers": outliers.shape[0],
        "Outlier Percentage": round(outlier_percentage, 2),
        "Outlier Indices": outliers.index.tolist()
    }


for feature, summary in outlier_summary.items():
    print(f"Feature: {feature}")
    print(f"  Lower Bound: {summary['Lower Bound']}")
    print(f"  Upper Bound: {summary['Upper Bound']}")
    print(f"  Number of Outliers: {summary['Number of Outliers']}")
    print(f"  Outlier Percentage: {summary['Outlier Percentage']}%")
    print(f"  Outlier Indices: {summary['Outlier Indices']}")
    print("-" * 50)

"""Official sites that can be used to cross verify the closeness of the outlier values or specific country related info:

World Bank: https://data.worldbank.org/

United Nations (UN): https://unstats.un.org/

World Health Organization (WHO): https://www.who.int/data

OECD Data: https://data.oecd.org/
"""

# Let's check for how normally the data is distributed by checking for the skewness and plot kde plots for all the numerical features
df.skew()

plt.figure(figsize=(12, 8))
for i, feature in enumerate(df.columns, 1):
    plt.subplot(3, 3, i)
    sns.kdeplot(df[feature], fill=True, color='red')
    plt.title(f'Distribution of {feature}')
    plt.xlabel(feature)
    plt.ylabel('Density')

plt.tight_layout()
plt.show()

"""**child_mort (1.45)** - Moderately right-skewed; many countries have lower child mortality rates, but some have exceptionally high rates.

**exports	(2.45)** -	Highly right-skewed; most countries have low export percentages relative to GDP, with a few outliers having very high values.

**health	(0.71)** -	Moderately right-skewed; healthcare spending is concentrated at lower values, with some countries having much higher values.

**imports	(1.91)** - Highly right-skewed; similar pattern to exports, with most countries having low import percentages.

**income	(2.23)** -	Highly right-skewed; indicates significant disparity in income distribution across countries.

**inflation	(5.15)** -	Very highly right-skewed; most countries have stable inflation, but a few have extreme inflation rates.

**life_expec	(-0.97)** -	Moderately left-skewed; most countries have high life expectancies, with a few having significantly lower values.

**total_fer	(0.97)** -	Moderately right-skewed; fertility rates are generally low, but some countries have higher rates.

**gdpp	(2.22)** -	Highly right-skewed; GDP per capita is concentrated at lower values, with a few high outliers.

### **Hypothesis testing**
"""

df.columns

# Let's perform the normality tests to decide suitable hypothesis tests that can be performed on the features
from scipy.stats import shapiro
import statsmodels.api as sm

# Shapiro-Wilk Test Results
normality_results = {}

for feature in df.columns:
    stat, p_value = shapiro(df[feature])
    normality_results[feature] = {
        "Test Statistic": round(stat, 4),
        "P-value": round(p_value, 4),
        "Normality": "Yes, the feature is normally distributed" if p_value > 0.05 else "No, the feature is not normally distributed"
    }

# Visualize QQ plots for normality check
plt.figure(figsize=(12, 8))
for i, feature in enumerate(df.columns, 1):
    plt.subplot(3, 3, i)
    sm.qqplot(df[feature], line='s', ax=plt.gca())
    plt.title(f"QQ Plot: {feature}")

plt.tight_layout()
plt.show()

normality_results

df.head()

"""#### **Hypothesis 1:** Relationship between Health Spending and Life Expectancy

**Null Hypothesis:**
There is no significant relationship between health spending (health) and life expectancy (life_expec).

**Alternative Hypothesis:** Higher health spending is associated with higher life expectancy.

**Test:**
Since both variables are continuous and not normally distributed let's perfom Spearman correlation to check for monotonic relationships.

"""

from scipy.stats import spearmanr

correlation, p_value = spearmanr(df['health'], df['life_expec'])

print("Spearman Correlation Coefficient:", correlation)
print("P-value:", p_value)

if p_value < 0.05:
    print("Reject the Null Hypothesis: There is a significant relationship between health spending and life expectancy.")
else:
    print("Fail to Reject the Null Hypothesis: There is no significant relationship between health spending and life expectancy.")



"""#### **Hypothesis 2:** Relationship between Fertility Rate and Income

**Null Hypothesis:** There is no significant relationship between fertility rate (total_fer) and income per person (income).

**Alternative Hypothesis:** Countries with higher fertility rates have lower income levels.

**Test:**
Since both variables are continuous and not normally distributed let's perfom Spearman correlation to check for monotonic relationships.

"""

correlation, p_value = spearmanr(df['total_fer'], df['income'])

print("Spearman Correlation Coefficient:", correlation)
print("P-value:", p_value)

if p_value < 0.05:
    print("Reject the Null Hypothesis: There is a significant relationship between fertility rate and income.")
else:
    print("Fail to Reject the Null Hypothesis: There is no significant relationship between fertility rate and income.")



"""####**Hypothesis 3:** Relationship between Income and Child Mortality

**Null Hypothesis:** There is no significant relationship between income (income) and child mortality (child_mort).

**Alternative Hypothesis:** Higher income levels are associated with lower child mortality rates.

**Test:**
Since child mortality and income are not normally distributed, correlation measures linear relationships for which we'll use the spearman correlation test, while the Mann-Whitney test compares groups i.e to compare child mortality rates between countries with income above and below the median.
"""

from scipy.stats import mannwhitneyu

# Spearman correlation test
correlation, p_value_corr = spearmanr(df['income'], df['child_mort'])

print("Spearman Correlation Coefficient (Income vs. Child Mortality):", correlation)
print("P-value (Spearman):", p_value_corr)

if p_value_corr < 0.05:
    print("Reject Null Hypothesis: Significant monotonic relationship between income and child mortality.")
else:
    print("Fail to Reject Null Hypothesis: No significant monotonic relationship between income and child mortality.")

# Divide data into groups based on median income
median_income = df['income'].median()
low_income_mortality = df.loc[df['income'] <= median_income, 'child_mort']
high_income_mortality = df.loc[df['income'] > median_income, 'child_mort']

# Mann-Whitney U Test
u_stat, p_value_mwu = mannwhitneyu(low_income_mortality, high_income_mortality, alternative='two-sided')

print("\nMann-Whitney U Test Results:")
print("U Statistic:", u_stat)
print("P-value (Mann-Whitney):", p_value_mwu)

if p_value_mwu < 0.05:
    print("Reject Null Hypothesis: Significant difference in child mortality rates between low and high-income groups.")
else:
    print("Fail to Reject Null Hypothesis: No significant difference in child mortality rates between low and high-income groups.")



"""####**Hypothesis 4:** Relationship between Inflation and GDP per Capita

**Null Hypothesis:** There is no significant relationship between inflation (inflation) and GDP per capita (gdpp).

**Alternative Hypothesis**: Higher inflation is associated with lower GDP per capita, indicating economic instability.

**Test:**
Since the features are not normally distributed we'll use Spearman correlation to assess the relationship and Mann-Whitney U Test to compare GDP per capita between high-inflation and low-inflation countries.
"""

# Spearman correlation test
correlation, p_value_corr = spearmanr(df['inflation'], df['gdpp'])

print("Spearman Correlation Coefficient (Inflation vs. GDP per Capita):", correlation)
print("P-value (Spearman):", p_value_corr)

if p_value_corr < 0.05:
    print("Reject Null Hypothesis: Significant monotonic relationship between inflation and GDP per capita.")
else:
    print("Fail to Reject Null Hypothesis: No significant monotonic relationship between inflation and GDP per capita.")

# Divide data into groups based on median inflation
median_inflation = df['inflation'].median()
low_inflation_gdpp = df.loc[df['inflation'] <= median_inflation, 'gdpp']
high_inflation_gdpp = df.loc[df['inflation'] > median_inflation, 'gdpp']

# Mann-Whitney U Test
u_stat, p_value_mwu = mannwhitneyu(low_inflation_gdpp, high_inflation_gdpp, alternative='two-sided')

print("\nMann-Whitney U Test Results:")
print("U Statistic:", u_stat)
print("P-value (Mann-Whitney):", p_value_mwu)

if p_value_mwu < 0.05:
    print("Reject Null Hypothesis: Significant difference in GDP per capita between low-inflation and high-inflation groups.")
else:
    print("Fail to Reject Null Hypothesis: No significant difference in GDP per capita between low-inflation and high-inflation groups.")



"""####**Hypothesis 5:** Relationship between Exports and GDP per Capita

**Null Hypothesis:** There is no significant relationship between exports (exports) and GDP per capita (gdpp).

**Alternative Hypothesis:** Higher exports are associated with higher GDP per capita.

**Test:**
Since the features are not normally distributed use Spearman correlation to assess the relationship.
"""

correlation, p_value = spearmanr(df['exports'], df['gdpp'])

print("Spearman Correlation Coefficient (Exports vs. GDP per Capita):", correlation)
print("P-value (Spearman):", p_value)

if p_value < 0.05:
    print("Reject Null Hypothesis: Significant monotonic relationship between exports and GDP per capita.")
else:
    print("Fail to Reject Null Hypothesis: No significant monotonic relationship between exports and GDP per capita.")



"""### **Data Preprocessing**"""

df.head()

"""#### **Feature Selection**"""

# For algorithms like KMeans and Heirarchical clustering, removing or combining features with high VIF (greater than 10 which was observed previously) is generally recommended
# to reduce multicollinearity and prevent it from distorting the clustering results. For DBSCAN, while it’s more robust to multicollinearity, addressing the issue can still lead to better results.

# Currently we have the following VIF for the features
vif_result = calculate_vif(df, df.columns)
vif_result.sort_values(by="VIF", ascending=False)

# Let's remove the first feature i.e life_expec
num_features = ['total_fer', 'imports', 'exports', 'income', 'health', 'gdpp', 'child_mort', 'inflation']
vif_result = calculate_vif(df, num_features)
vif_result.sort_values(by="VIF", ascending=False)

# Let's now remove the imports feature
num_features = ['total_fer', 'exports', 'income', 'health', 'gdpp', 'child_mort', 'inflation']
vif_result = calculate_vif(df, num_features)
vif_result.sort_values(by="VIF", ascending=False)

# Let's now remove the total_fer feature
num_features = ['exports', 'income', 'health', 'gdpp', 'child_mort', 'inflation']
vif_result = calculate_vif(df, num_features)
vif_result.sort_values(by="VIF", ascending=False)

# Lastly we'll remove the income feature as gdpp will also give similar sort of info
num_features = ['exports', 'health', 'gdpp', 'child_mort', 'inflation']
vif_result = calculate_vif(df, num_features)
vif_result.sort_values(by="VIF", ascending=False)

# We can use the below set of features for KMeans and Heirarchical clustering mainly

"""#### **Outlier Handling**"""

df_limited_features = df[num_features]
df_limited_features.head()

# Since there are no missing values in this dataset, we'll proceed further to remove outliers present in the numerical features using IQR method

Q1 = df_limited_features.quantile(0.25)
Q3 = df_limited_features.quantile(0.75)
IQR = Q3 - Q1

# Filter out outliers
df_no_outliers = df_limited_features[~((df_limited_features < (Q1 - 1.5 * IQR)) | (df_limited_features > (Q3 + 1.5 * IQR))).any(axis=1)]

df_no_outliers.shape     # this dataframe consists no outliers which can be used for KMeans algorithm which is highly sensitive to outliers and also for Heirarchical clustering which is moderately
                         # sensitive to the outliers and therefore removing them can improve the clustering performance of the models

df_final = df_no_outliers.copy()

# Since gdpp, child_mort and inflation still show some skewness, we can apply boxcox transformations on those features to adjust the distribution of individual features to make them more symmetric,
# reducing the effect of skewnes  but boxcox strictly requires all values to be positive
df_no_outliers.skew()

# check if all the values are positive in df_no_outliers
df_no_outliers.apply(lambda x: (x > 0).all())

df_no_outliers.loc[df_no_outliers['inflation'] < 0]               # Since inflation has few negative values let's shift those values to make them positive and then apply boxcox transformation on it

min_inflation = df_no_outliers['inflation'].min()
if min_inflation <= 0:
    shift_value = abs(min_inflation) + 1  # Shift to make all values positive
    df_no_outliers['inflation_shifted'] = df_no_outliers['inflation'] + shift_value
    df_no_outliers.drop(columns=['inflation'], inplace=True)
    df_no_outliers.rename(columns={'inflation_shifted': 'inflation'}, inplace=True)

from scipy.stats import boxcox

# Initialize an empty dictionary to store lambda values
lambdas = {}
features_to_transform = ['gdpp', 'child_mort', 'inflation']

# Apply Box-Cox transformation
for feature in features_to_transform:
    df_no_outliers[f'{feature}_boxcox'], lambdas[feature] = boxcox(df_no_outliers[feature] + 1)  # Add 1 to handle zeros

for feature, lambda_value in lambdas.items():
    print(f"Lambda for {feature}: {lambda_value}")

df_no_outliers.drop(columns=['gdpp', 'child_mort', 'inflation'], inplace=True)
df_no_outliers.skew()

df_no_outliers.isna().sum()            # No missing values detected after applying boxcox transformation

"""#### **Scaling Features**
**K-Means** and **Hieararchical clustering** is a distance-based algorithm. Because of that, it’s really important to perform feature scaling (normalize, standardize, or choose any other option in which the distance has some comparable meaning for all the columns).

For our use case, we can use MinMaxScaler instead of StandardScaler, transforming the feature values to fall within the bounded intervals (min and max), rather than making them to fall around mean as 0 with standard deviation as 1 (StandardScaler).

MinMaxScaler is an excellent tool for this purpose. MinMaxScaler scales all the data features in the range [0, 1] or else in the range [-1, 1] if there are negative values in the dataset.
"""

from sklearn.preprocessing import MinMaxScaler

# Normalization for KMeans and Hierarchical Clustering
scaler = MinMaxScaler()
scaler.fit(df_no_outliers)
df_scaled = scaler.transform(df_no_outliers)
df_scaled = pd.DataFrame(df_scaled, columns=df_no_outliers.columns, index=df_no_outliers.index)

df_scaled.head()    # We'll use this dataframe for KMeans and Hierarchical clustering

"""### **KMeans Clustering**
when optimal number of clusters are 3
"""

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# Let's use the Elbow Method to determine the optimal number of clusters
inertia = []
silhouette_scores = []

for k in range(2, 10):  # Test for cluster counts from 2 to 9, and initialize with k-means++ as it uses a smarter initialization method which is also the default value
    kmeans = KMeans(init="k-means++", n_clusters=k, random_state=42)
    kmeans.fit(df_scaled)
    inertia.append(kmeans.inertia_)
    silhouette_scores.append(silhouette_score(df_scaled, kmeans.labels_))

# Plot Elbow Method results
plt.figure(figsize=(8, 4))
plt.plot(range(2, 10), inertia, marker='o')
plt.title('Elbow Method')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS (Within Cluster Sum of Squares)')

optimal_k = 3
plt.plot(optimal_k, inertia[optimal_k - 2], 'ro')  # Red marker at the elbow point
plt.annotate(f'Optimal k = {optimal_k}',
             xy=(optimal_k, inertia[optimal_k - 2]),
             xytext=(optimal_k + 0.5, inertia[optimal_k - 2] + 0.05),
             arrowprops=dict(arrowstyle='->', lw=1.5))
plt.show()

# Plot Silhouette Scores to evaluate the clustering performance
plt.figure(figsize=(8, 4))
plt.plot(range(2, 10), silhouette_scores, marker='o', color='green')
plt.title('Silhouette Scores')
plt.xlabel('Number of Clusters')
plt.ylabel('Silhouette Score')
plt.show()

print(f"Elbow Method Results: {inertia}")
print(f"Silhouette Scores: {silhouette_scores}")

"""The Silhouette Score is a metric used to evaluate the quality of clustering. It measures how well each data point is assigned to its cluster compared to other clusters, based on distances. The score ranges from -1 to 1, where:

1: The data point is well-matched to its own cluster and poorly matched to other clusters.

0: The data point is on or near the decision boundary between two clusters.

-1: The data point is likely assigned to the wrong cluster.

The "elbow" is most noticeable at 3 clusters, suggesting this is the optimal number.

The highest silhouette score is at 2 clusters (0.296), but this might oversimplify the data structure.

The score decreases from 2 to 4 clusters but remains relatively stable after 4 clusters.
Beyond 4 clusters, the Silhouette Score dips further, indicating less distinct clusters.

So, choosing 2 clusters gives the highest silhouette score but oversimplifies the structure.
Choosing **3 clusters** balances WCSS reduction and clustering quality, making it a reasonable choice.
"""

# Now fit KMeans with the chosen number of clusters
kmeans_3 = KMeans(n_clusters=optimal_k, random_state=42)
kmeans_3.fit(df_scaled)

kmeans_clusters_3 = kmeans_3.labels_

"""### **Hierarchical Clustering**
when optimal number of clusters are 3
"""

from sklearn.cluster import AgglomerativeClustering
import scipy.cluster.hierarchy as sch

# Dendrogram to visualize clustering
plt.figure(figsize=(10, 7))
dendrogram = sch.dendrogram(sch.linkage(df_scaled, method='ward', metric='euclidean'))
plt.axhline(y=3, color='r', linestyle='--')
plt.title('Dendrogram')
plt.xlabel('Data Points')
plt.ylabel('Euclidean Distance')
plt.show()

# Let's fit Agglomerative Clustering with the chosen number of clusters
hierarchical_3 = AgglomerativeClustering(n_clusters=optimal_k, linkage='ward', metric='euclidean')
hierarchical_clusters_3 = hierarchical_3.fit_predict(df_scaled)

"""To determine the optimal number of clusters, we look for the largest vertical distance between two horizontal lines, where cutting the dendrogram would split it into distinct clusters.

If we apply a horizontal cut at around height = 3, the dendrogram would split into **3** clusters.
"""

df_scaled['kmeans_clusters_3'] = kmeans_clusters_3
df_scaled['hierarchical_clusters_3'] = hierarchical_clusters_3

df_scaled.head()

pd.crosstab(df_scaled['kmeans_clusters_3'], df_scaled['hierarchical_clusters_3'])

kmeans_profile = df_scaled.drop('hierarchical_clusters_3', axis=1).groupby('kmeans_clusters_3').mean()
hierarchical_profile = df_scaled.drop('kmeans_clusters_3', axis=1).groupby('hierarchical_clusters_3').mean()

print("KMeans Cluster Profiles:\n", kmeans_profile)
print("\n\n")
print("Hierarchical Cluster Profiles:\n", hierarchical_profile)

df_scaled.head()

"""### **Visualize the Clusters using PCA and Pairplots**
when optimal number of clusters are 3
"""

from sklearn.decomposition import PCA

X = df_scaled.drop(columns=['kmeans_clusters_3', 'hierarchical_clusters_3'])
pca = PCA(n_components=2, random_state=42)
pca_data = pca.fit_transform(X)

plt.figure(figsize=(10, 6))
plt.scatter(pca_data[:, 0], pca_data[:, 1], c=df_scaled['kmeans_clusters_3'], cmap='viridis', label='KMeans Clusters')
plt.title('KMeans Clusters (PCA Visualization)')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.colorbar()
plt.show()

plt.figure(figsize=(10, 6))
plt.scatter(pca_data[:, 0], pca_data[:, 1], c=df_scaled['hierarchical_clusters_3'], cmap='viridis', label='Hierarchical Clusters')
plt.title('Hierarchical Clusters (PCA Visualization)')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.colorbar()
plt.show()

plt.figure(figsize=(10,8))
sns.pairplot(df_scaled, hue='kmeans_clusters_3', vars=['exports', 'health', 'gdpp_boxcox', 'child_mort_boxcox', 'inflation_boxcox'])
plt.show()

plt.figure(figsize=(10,8))
sns.pairplot(df_scaled, hue='hierarchical_clusters_3', vars=['exports', 'health', 'gdpp_boxcox', 'child_mort_boxcox', 'inflation_boxcox'])
plt.show()

"""**Let's evaluate the cluster quality**"""

from sklearn.metrics import silhouette_score

kmeans_silhouette = silhouette_score(X, df_scaled['kmeans_clusters_3'])
hierarchical_silhouette = silhouette_score(X, df_scaled['hierarchical_clusters_3'])

print("KMeans Silhouette Score:", kmeans_silhouette)
print("Hierarchical Clustering Silhouette Score:", hierarchical_silhouette)

"""####**Insights**
####**Cluster Overlapping:**
**KMeans vs. Hierarchical:**

Significant overlap between the two methods:

* KMeans cluster 2 overlaps heavily with Hierarchical cluster 0 (57 samples).

* KMeans cluster 0 overlaps with Hierarchical cluster 2 (25 samples).

* Minimal overlap for other clusters.

####**Cluster Discrepancy:**

**Silhouette Scores:**

* KMeans (0.260): Indicates moderate cluster separation.

* Hierarchical (0.239): Shows slightly weaker cluster cohesion than KMeans.
Insights from Cluster Profiles:

**Insights from Cluster Profiles:**

**KMeans Clusters:**

Cluster 0: High GDP per capita, high health, low child mortality (developed regions).

Cluster 1: Low GDP per capita, low health, high child mortality (least developed regions).

Cluster 2: Moderate GDP per capita, moderate health, average child mortality (developing regions).

**Hierarchical Clusters:**

Similar patterns to KMeans but with slightly less differentiation:


Cluster 2 aligns with KMeans cluster 0.

Cluster 1 aligns with KMeans cluster 1.
"""

df_scaled.drop(['kmeans_clusters_3', 'hierarchical_clusters_3'], axis=1, inplace=True)

"""### **KMeans Clustering**
when optimal number of clusters are 4
"""

inertia = []

for k in range(2, 10):  # Test for cluster counts from 2 to 9, and initialize with k-means++ as it uses a smarter initialization method which is also the default value
    kmeans = KMeans(init="k-means++", n_clusters=k, random_state=42)
    kmeans.fit(df_scaled)
    inertia.append(kmeans.inertia_)
    silhouette_scores.append(silhouette_score(df_scaled, kmeans.labels_))

# Plot Elbow Method results
plt.figure(figsize=(8, 4))
plt.plot(range(2, 10), inertia, marker='o')
plt.title('Elbow Method')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS (Within Cluster Sum of Squares)')

# let's set the optimal cluster value to 4 and check for the clusters
optimal_k_new = 4
plt.plot(optimal_k_new, inertia[optimal_k_new - 2], 'ro')  # Red marker at the elbow point
plt.annotate(f'Optimal k = {optimal_k_new}',
             xy=(optimal_k_new, inertia[optimal_k_new - 2]),
             xytext=(optimal_k_new + 0.5, inertia[optimal_k_new - 2] + 0.05),
             arrowprops=dict(arrowstyle='->', lw=1.5))
plt.show()

# KMeans clustering
kmeans_4 = KMeans(n_clusters=optimal_k_new, random_state=42)
kmeans_4.fit(df_scaled)

kmeans_clusters_4 = kmeans_4.labels_

"""### **Hierarchical Clustering**
when optimal number of clusters are 4
"""

# Dendrogram to visualize clustering
plt.figure(figsize=(10, 7))
dendrogram = sch.dendrogram(sch.linkage(df_scaled, method='ward', metric='euclidean'))
plt.axhline(y=2, color='r', linestyle='--')
plt.title('Dendrogram')
plt.xlabel('Data Points')
plt.ylabel('Euclidean Distance')
plt.show()


# Let's fit Agglomerative Clustering with the chosen number of clusters i.e 4
hierarchical_4 = AgglomerativeClustering(n_clusters=optimal_k_new, linkage='ward', metric='euclidean')
hierarchical_clusters_4 = hierarchical_4.fit_predict(df_scaled)

df_scaled['kmeans_clusters_4'] = kmeans_clusters_4
df_scaled['hierarchical_clusters_4'] = hierarchical_clusters_4

df_scaled.head()

pd.crosstab(df_scaled['kmeans_clusters_4'], df_scaled['hierarchical_clusters_4'])

kmeans_profile = df_scaled.drop('hierarchical_clusters_4', axis=1).groupby('kmeans_clusters_4').mean()
hierarchical_profile = df_scaled.drop('kmeans_clusters_4', axis=1).groupby('hierarchical_clusters_4').mean()

print("KMeans Cluster Profiles:\n", kmeans_profile)
print("\n\n")
print("Hierarchical Cluster Profiles:\n", hierarchical_profile)

"""### **Visualize the Clusters using PCA and Pairplots**
when optimal number of clusters are 4
"""

X = df_scaled.drop(columns=['kmeans_clusters_4', 'hierarchical_clusters_4'])
pca = PCA(n_components=2, random_state=42)
pca_data = pca.fit_transform(X)

plt.figure(figsize=(10, 6))
plt.scatter(pca_data[:, 0], pca_data[:, 1], c=df_scaled['kmeans_clusters_4'], cmap='viridis', label='KMeans Clusters')
plt.title('KMeans Clusters (PCA Visualization)')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.colorbar()
plt.show()

plt.figure(figsize=(10, 6))
plt.scatter(pca_data[:, 0], pca_data[:, 1], c=df_scaled['hierarchical_clusters_4'], cmap='viridis', label='Hierarchical Clusters')
plt.title('Hierarchical Clusters (PCA Visualization)')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.colorbar()
plt.show()

plt.figure(figsize=(10,8))
sns.pairplot(df_scaled, hue='kmeans_clusters_4', vars=['exports', 'health', 'gdpp_boxcox', 'child_mort_boxcox', 'inflation_boxcox'])
plt.show()

plt.figure(figsize=(10,8))
sns.pairplot(df_scaled, hue='hierarchical_clusters_4', vars=['exports', 'health', 'gdpp_boxcox', 'child_mort_boxcox', 'inflation_boxcox'])
plt.show()

kmeans_silhouette = silhouette_score(X, df_scaled['kmeans_clusters_4'])
hierarchical_silhouette = silhouette_score(X, df_scaled['hierarchical_clusters_4'])

print("KMeans Silhouette Score:", kmeans_silhouette)
print("Hierarchical Clustering Silhouette Score:", hierarchical_silhouette)

"""####**Insights**
####**Cluster Overlapping:**
**KMeans vs. Hierarchical:**

Overlap increases:

* KMeans cluster 3 overlaps with Hierarchical cluster 1 (28 samples).

* KMeans cluster 2 overlaps with Hierarchical clusters 2 and 3.

* KMeans cluster 1 aligns entirely with Hierarchical cluster 0 (38 samples).

####**Cluster Discrepancy:**
**Silhouette Scores:**

* KMeans (0.244): Slight drop in cohesion compared to 3 clusters, indicating weaker cluster separation.

* Hierarchical (0.228): Also shows weaker cluster cohesion with 4 clusters compared to 3.

**Insights from Cluster Profiles:**

**KMeans Clusters:**

Cluster 0: High GDP per capita, high health, low child mortality (developed regions).

Cluster 1: Low GDP per capita, low health, high child mortality (least developed regions).

Cluster 2: Moderate GDP per capita, moderate health, low inflation (developing regions).

Cluster 3: High exports, high inflation, moderate GDP per capita (export-driven economies).

**Hierarchical Clusters:**

Similar patterns but with reduced differentiation:

Cluster 2 aligns with KMeans cluster 0.

Cluster 1 aligns with KMeans cluster 3.
"""

df_scaled.drop(['kmeans_clusters_4', 'hierarchical_clusters_4'], axis=1, inplace=True)

df_scaled.head()

from sklearn.metrics import davies_bouldin_score, calinski_harabasz_score

X = df_scaled

# Davies-Bouldin Index
dbi_kmeans_3 = davies_bouldin_score(X, kmeans_clusters_3)
dbi_kmeans_4 = davies_bouldin_score(X, kmeans_clusters_4)
dbi_hierarchical_3 = davies_bouldin_score(X, hierarchical_clusters_3)
dbi_hierarchical_4 = davies_bouldin_score(X, hierarchical_clusters_4)

# Calinski-Harabasz Score
ch_kmeans_3 = calinski_harabasz_score(X, kmeans_clusters_3)
ch_kmeans_4 = calinski_harabasz_score(X, kmeans_clusters_4)
ch_hierarchical_3 = calinski_harabasz_score(X, hierarchical_clusters_3)
ch_hierarchical_4 = calinski_harabasz_score(X, hierarchical_clusters_4)

print("Davies-Bouldin Index (KMeans 3):", dbi_kmeans_3)
print("Davies-Bouldin Index (KMeans 4):", dbi_kmeans_4)
print("Davies-Bouldin Index (Hierarchical 3):", dbi_hierarchical_3)
print("Davies-Bouldin Index (Hierarchical 4):", dbi_hierarchical_4)
print("\n")
print("Calinskli-Harabasz Score (KMeans 3):", ch_kmeans_3)
print("Calinskli-Harabasz Score (KMeans 4):", ch_kmeans_4)
print("Calinskli-Harabasz Score (Hierarchical 3):", ch_hierarchical_3)
print("Calinskli-Harabasz Score (Hierarchical 4):", ch_hierarchical_4)

"""Davies-Bouldin Index (DBI):
Lower values indicate better clustering

Calinski-Harabasz Score (CHS):
Higher values indicate better-defined clusters.

Based on DBI and CHS, the 3-cluster solution is better for both KMeans and Hierarchical clustering

**Recommendation**

Number of Clusters: 3 clusters is preferable.
* Better silhouette scores and clearer cluster boundaries.
* Stronger differentiation of clusters (GDP, health, child mortality).
* Minimal overlap compared to 4 clusters.

Clustering Method: **KMeans** offers better cohesion and separation than Hierarchical Clustering in both cases.

### **DBSCAN (Density Based Spatial Clustering of Applications with Noise)**
"""

df_dbscan_scaled = df_scaled.copy()

df_dbscan_scaled.head()

"""DBSCAN relies on two main parameters:

eps (Epsilon): The maximum distance between two samples for them to be considered as part of the same neighborhood.

min_samples: The minimum number of samples required to form a dense region (cluster).
"""

from sklearn.neighbors import NearestNeighbors

# Step 1: Determining the optimal eps using the elbow method
def plot_k_distance(data, k=4):
    neigh = NearestNeighbors(n_neighbors=k)
    neigh.fit(data)
    distances, _ = neigh.kneighbors(data)
    distances = np.sort(distances[:, k - 1], axis=0)

    # Plot the k-distance graph
    plt.figure(figsize=(8, 6))
    plt.plot(distances)
    plt.title("k-Distance Graph", fontsize=14)
    plt.xlabel("Points sorted by distance", fontsize=12)
    plt.ylabel(f"{k}-th Nearest Neighbor Distance", fontsize=12)
    plt.grid()
    plt.show()

plot_k_distance(df_dbscan_scaled, k=10)

"""Value of  **min_samples** should be greater than or equal to  d+1 ; where  d  is dimensionality of the data and a
lot of libraries use the value of  **min_samples**  approximately equal to  2*d


From the above K-Distance graph we can see that the elbow seems to be created when the eps value is around 0.58 which can be considered as an optimal value when k=10 (considering 2*d), but we will check for the best possible eps and min_samples value by computing the silhouette scores and see if its close to the one we observed in the above graph
"""

from sklearn.cluster import DBSCAN

# Step 2: Experiment with eps and min_samples
eps_values = np.arange(0.1, 10, 0.05)
min_samples_values = range(5, 31)
optimal_clusters = None
best_params = None
best_silhouette = -1
distance_metrics = ['euclidean', 'manhattan']

# Initialize a dictionary to store the best results for each cluster count (2 to 10)
best_results_per_cluster = {n: {'eps': None, 'min_samples': None, 'silhouette': -1, 'distance_metric': None} for n in range(2, 11)}

results = []  # List to store detailed results

for metric in distance_metrics:
    for eps in eps_values:
        for min_samples in min_samples_values:
          dbscan = DBSCAN(eps=eps, min_samples=min_samples)
          labels = dbscan.fit_predict(df_dbscan_scaled)

          # ensure at least 2 clusters
          number_of_clusters = len(set(labels))
          if number_of_clusters >= 2 and number_of_clusters <= 10:  # 2 to 10 clusters
              silhouette = silhouette_score(df_dbscan_scaled, labels, metric=metric)
              results.append((eps, min_samples, silhouette, number_of_clusters))

              if silhouette > best_silhouette:
                  best_silhouette = round(silhouette, 3)
                  best_params = (round(eps, 3), min_samples)
                  optimal_clusters = number_of_clusters

              # Update best results for each cluster count
              if silhouette > best_results_per_cluster[number_of_clusters]['silhouette']:
                  best_results_per_cluster[number_of_clusters] = {
                      'eps': eps,
                      'min_samples': min_samples,
                      'silhouette': silhouette,
                      'distance_metric': metric
                  }

# Storing the best results for each cluster count into a DataFrame
results_list = []
for n_clusters, params in best_results_per_cluster.items():
    if params['silhouette']:
        results_list.append({
            'n_clusters': n_clusters,
            'eps': params['eps'],
            'min_samples': params['min_samples'],
            'silhouette_score': params['silhouette'],
            'distance_metric': params['distance_metric']
        })

results_df = pd.DataFrame(results_list)

if best_params:
    print(f"\nBest Parameters: eps={best_params[0]}, min_samples={best_params[1]}")
    print(f"Best Silhouette Score: {best_silhouette:.4f}")
    print(f"Optimal Number of Clusters: {optimal_clusters}")
else:
    print("No valid clustering found.")

"""From the above results after computing the Silhouette scores for various combinations of hyperparameters i.e epsilon, minimum samples and distance metric passed to the DBSCAN model for predictions, it is observed that the highest Silhouette score obtained is when the **optimal number of clusters were 2 (0.24).**

So in comparison with the Silhouette scores of KMeans and Hierarchical clustering algorithms **(for 3 clusters: KMeans Silhouette Score: 0.260,
Hierarchical Clustering Silhouette Score: 0.239 and for 4 clusters:
KMeans Silhouette Score: 0.243, Hierarchical Clustering Silhouette Score: 0.228)**

It is evident that the **KMeans Model** has performed well for 3 clusters with the highest Silhouette score.

But let's confirm it by checking for the Silhouette scores for DBSCAN when the number of clusters were 3 and above which are valid.
"""

results_df

"""From the above results for DBSCAN algorithm, it is clearly evident that when the clusters are 3 and 6 (which gave valid results), the Silhouette scores are low or the DBSCAN model didn't perform well in forming clusters as the value closer to zero indicates the samples\data points are on the decision boundaries between two clusters.

Yet we'll try visualize the clustering when the clusters were 2,3 and 6
"""

best_params

# Step 3: Visualize the clusters formed
best_eps, best_min_samples = best_params
dbscan_2 = DBSCAN(eps=best_eps, min_samples=best_min_samples)
dbscan_labels_2 = dbscan_2.fit_predict(df_dbscan_scaled)

from sklearn.decomposition import PCA
from sklearn.manifold import TSNE

# Apply PCA for 2D visualization
pca = PCA(n_components=2, random_state=42)
X_pca = pca.fit_transform(df_dbscan_scaled)

# Apply t-SNE for 2D visualization
tsne = TSNE(n_components=2, random_state=42)
X_tsne = tsne.fit_transform(df_dbscan_scaled)

# Define unique labels and colors
unique_labels = set(dbscan_labels_2)
colors = plt.cm.viridis(np.linspace(0, 1, len(unique_labels)))

# Visualizing clusters using PCA
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
for label, color in zip(unique_labels, colors):
    mask = dbscan_labels_2 == label
    label_name = f"Cluster {label}" if label != -1 else "Noise"
    plt.scatter(X_pca[mask, 0], X_pca[mask, 1], c=[color], label=label_name, edgecolors='k', s=50)
plt.title('DBSCAN Clusters (PCA)')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.legend(title="Clusters")

# Visualizing clusters using t-SNE
plt.subplot(1, 2, 2)
for label, color in zip(unique_labels, colors):
    mask = dbscan_labels_2 == label
    label_name = f"Cluster {label}" if label != -1 else "Noise"
    plt.scatter(X_tsne[mask, 0], X_tsne[mask, 1], c=[color], label=label_name, edgecolors='k', s=50)
plt.title('DBSCAN Clusters (t-SNE)')
plt.xlabel('t-SNE Component 1')
plt.ylabel('t-SNE Component 2')
plt.legend(title="Clusters")

plt.tight_layout()
plt.show()

"""THe above visualization forms basically only 1 cluster that comprises of most of the data points and the remaining are considered as noise points."""

dbscan_3_clusters = results_df.loc[results_df['n_clusters'] == 3].sort_values(by='silhouette_score', ascending=False).head(1)
dbscan_3_clusters

# Let's check the best hyperparameters for 3 clusters obtained using DBSCAN
best_eps, best_min_samples = dbscan_3_clusters['eps'].values[0], int(dbscan_3_clusters['min_samples'].values[0])
dbscan_3 = DBSCAN(eps=best_eps, min_samples=best_min_samples)
dbscan_labels_3 = dbscan_3.fit_predict(df_dbscan_scaled)

# Apply PCA for 2D visualization
pca = PCA(n_components=2, random_state=42)
X_pca = pca.fit_transform(df_dbscan_scaled)

# Apply t-SNE for 2D visualization
tsne = TSNE(n_components=2, random_state=42)
X_tsne = tsne.fit_transform(df_dbscan_scaled)

# Define unique labels and colors
unique_labels = set(dbscan_labels_3)
colors = plt.cm.viridis(np.linspace(0, 1, len(unique_labels)))

# Visualizing clusters using PCA
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
for label, color in zip(unique_labels, colors):
    mask = dbscan_labels_3 == label
    label_name = f"Cluster {label}" if label != -1 else "Noise"
    plt.scatter(X_pca[mask, 0], X_pca[mask, 1], c=[color], label=label_name, edgecolors='k', s=50)
plt.title('DBSCAN Clusters (PCA)')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.legend(title="Clusters")

# Visualizing clusters using t-SNE
plt.subplot(1, 2, 2)
for label, color in zip(unique_labels, colors):
    mask = dbscan_labels_3 == label
    label_name = f"Cluster {label}" if label != -1 else "Noise"
    plt.scatter(X_tsne[mask, 0], X_tsne[mask, 1], c=[color], label=label_name, edgecolors='k', s=50)
plt.title('DBSCAN Clusters (t-SNE)')
plt.xlabel('t-SNE Component 1')
plt.ylabel('t-SNE Component 2')
plt.legend(title="Clusters")

plt.tight_layout()
plt.show()

"""The number of noise points are many compared to the previous visualization (2 clusters), but has grouped the countries into two main clusters."""

dbscan_6_clusters = results_df.loc[results_df['n_clusters'] == 6].sort_values(by='silhouette_score', ascending=False).head(1)
dbscan_6_clusters

# Let's check the best hyperparameters for 6 clusters obtained using DBSCAN
best_eps, best_min_samples = dbscan_6_clusters['eps'].values[0], int(dbscan_6_clusters['min_samples'].values[0])
dbscan_6 = DBSCAN(eps=best_eps, min_samples=best_min_samples)
dbscan_labels_6= dbscan_6.fit_predict(df_dbscan_scaled)

np.unique(dbscan_labels_6)

# Apply PCA for 2D visualization
pca = PCA(n_components=2, random_state=42)
X_pca = pca.fit_transform(df_dbscan_scaled)

# Apply t-SNE for 2D visualization
tsne = TSNE(n_components=2, random_state=42)
X_tsne = tsne.fit_transform(df_dbscan_scaled)

# Define unique labels and colors
unique_labels = set(dbscan_labels_6)
colors = plt.cm.viridis(np.linspace(0, 1, len(unique_labels)))

# Visualizing clusters using PCA
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
for label, color in zip(unique_labels, colors):
    mask = dbscan_labels_6 == label
    label_name = f"Cluster {label}" if label != -1 else "Noise"
    plt.scatter(X_pca[mask, 0], X_pca[mask, 1], c=[color], label=label_name, edgecolors='k', s=50)
plt.title('DBSCAN Clusters (PCA)')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.legend(title="Clusters")

# Visualizing clusters using t-SNE
plt.subplot(1, 2, 2)
for label, color in zip(unique_labels, colors):
    mask = dbscan_labels_6 == label
    label_name = f"Cluster {label}" if label != -1 else "Noise"
    plt.scatter(X_tsne[mask, 0], X_tsne[mask, 1], c=[color], label=label_name, edgecolors='k', s=50)
plt.title('DBSCAN Clusters (t-SNE)')
plt.xlabel('t-SNE Component 1')
plt.ylabel('t-SNE Component 2')
plt.legend(title="Clusters")

plt.tight_layout()
plt.show()

"""Here most of the points are noise points and few points are divided into 5 main clusters which also seems to show some overlapping with other clusters in the PCA visualization.

### **Model Comparison Summary:**

**KMeans:**
  * Interpretability: High
  * Compactness: 0.26
  * Noise Handling: Poor

**Hierarchical:**
  * Interpretability: Moderate
  * Compactness: 0.239
  * Noise Handling: Poor

**DBSCAN:**
  * Interpretability: Low
  * Compactness: 0.119
  * Noise Handling: Good

Based on the above results considering for 3 clusters as the optimal value (choosing 2 clusters will oversimplify the structure of our data), we can conclude that **KMeans** model performs well with a good Interpretability with 3 clusters and better compactness when compared to other models, but has a poor noise handling.
"""

# Since we have trained our models using just 5 features by avoiding multicollinearity incase of using 9 features, we can use the below dataframe which consists of raw data without outliers and boxcox
# present, also after deployment the user can give input related to these 5 features to get the result.
df_final.head()

df_final.shape

from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=3, init="k-means++", random_state=42)
kmeans.fit(df_scaled)
kmeans_labels = kmeans.labels_

df_final['cluster'] = kmeans_labels
df_final.head()

df_final['cluster'].value_counts()

"""### **Cluster-wise analysis for labelling**"""

cluster_0 = df_final.loc[df_final['cluster'] == 0]
cluster_0.head(2)

cluster_0.describe()

"""**Cluster 0: Highly Developed Country**
* GDP per capita (gdpp): Highest average (mean: 14,934), with a maximum of 30,800.
* Child Mortality (child_mort): Very low (mean: 8.28, max: 19.7).
* Inflation: Relatively stable (mean: 1.97), with some negative inflation (deflation).
* Exports and Health: High exports (mean: 47.09) and good health investment (mean: 7.87).

These characteristics suggest strong economic stability, low mortality, and high development.
"""

cluster_1 = df_final.loc[df_final['cluster'] == 1]
cluster_1.head(2)

cluster_1.describe()

"""**Cluster 1: Least Developed Country**
* GDP per capita (gdpp): Lowest average (mean: 815), with a maximum of 1,490.
* Child Mortality (child_mort): Extremely high (mean: 83.83, max: 137).
* Inflation: High and volatile (mean: 8.64, max: 23.6).
* Exports and Health: Low exports (mean: 23.81) and health investment (mean: 6.11).

These indicators point to economic challenges, high mortality, and underdevelopment.

These are the countries that require the direst need of aid.
"""

cluster_2 = df_final.loc[df_final['cluster'] == 2]
cluster_2.head(2)

cluster_2.describe()

"""**Cluster 2: Moderately Developed Country**
* GDP per capita (gdpp): Intermediate (mean: 5,328), with a maximum of 19,300.
* Child Mortality (child_mort): Moderate (mean: 28.04, max: 119).
* Inflation: High (mean: 9.25), indicating some economic instability.
* Exports and Health: Moderate exports (mean: 40.96) and health investment (mean: 5.74).

This cluster represents countries that are developing but still face significant challenges in key areas.
"""

# Map clusters to meaningful labels
cluster_labels = {
    0: "Highly Developed Country",
    1: "Least Developed Country",
    2: "Moderately Developed Country"
}

df_final['label'] = df_final['cluster'].map(cluster_labels)
df_final.tail()

# Final visualization of the countries

# Assign colors and markers for each cluster
cluster_colors = ['green', 'red', 'blue']
cluster_markers = ['o', '^', 's']

# PCA
pca = PCA(n_components=2, random_state=42)
X_pca = pca.fit_transform(df_scaled)

plt.figure(figsize=(14, 7))

# PCA Visualization
plt.subplot(1, 2, 1)
for cluster_id in range(3):
    plt.scatter(
        X_pca[kmeans_labels == cluster_id, 0],
        X_pca[kmeans_labels == cluster_id, 1],
        c=cluster_colors[cluster_id],
        marker=cluster_markers[cluster_id],
        label=cluster_labels[cluster_id],
        edgecolors='k',
        s=100
    )
plt.title('KMeans Clusters (PCA)')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.legend(title="Cluster Type", loc="best")

# t-SNE
tsne = TSNE(n_components=2, random_state=42)
X_tsne = tsne.fit_transform(df_scaled)

# t-SNE Visualization
plt.subplot(1, 2, 2)
for cluster_id in range(3):
    plt.scatter(
        X_tsne[kmeans_labels == cluster_id, 0],
        X_tsne[kmeans_labels == cluster_id, 1],
        c=cluster_colors[cluster_id],
        marker=cluster_markers[cluster_id],
        label=cluster_labels[cluster_id],
        edgecolors='k',
        s=100
    )
plt.title('KMeans Clusters (t-SNE)')
plt.xlabel('t-SNE Component 1')
plt.ylabel('t-SNE Component 2')
plt.legend(title="Cluster Type", loc="best")

plt.tight_layout()
plt.show()

# Group by 'label' and aggregate countries as a list, and also get the count
country_count_per_label = df_final.reset_index().groupby('label').agg(
    countries=('country', list),
    count=('country', 'size')
).reset_index()

country_count_per_label

country_count_per_label.loc[0, 'countries']

country_count_per_label.loc[1, 'countries']

country_count_per_label.loc[2, 'countries']

df_final.head()

"""### **The top 10 Least Developed Countries**
These countries require the direst need of aid.
"""

df_final.loc[df_final['label'] == 'Least Developed Country'].sort_values(by=['child_mort', 'gdpp'], ascending=[False, True]).head(10)

"""### **The top 10 Highly Developed Countries**
The NGOs can focus less on these countries for now
"""

df_final.loc[df_final['label'] == 'Highly Developed Country'].sort_values(by=['child_mort', 'gdpp'], ascending=[True, False]).head(10)

"""## **Save the model**"""

# Let's store the lower bound and upper bound values for the required features for raw input data comparison
Q1 = df_limited_features.quantile(0.25)
Q3 = df_limited_features.quantile(0.75)
IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

outlier_bounds = {
    feature: {
        'lower_bound': round(lower_bound[feature], 3),
        'upper_bound': round(upper_bound[feature], 3)
    }
    for feature in df_limited_features.columns
}

print("Outlier Bounds Dictionary:")
outlier_bounds

# Let's store the lamdas values that we initially used for our box transformation of our data
lambdas

import pickle

# Lets save all the necessary items required for processing the raw user input data
with open('outlier_bounds.pkl', 'wb') as f:
    pickle.dump(outlier_bounds, f)

with open('lambdas.pkl', 'wb') as f:
    pickle.dump(lambdas, f)

with open('scaler.pkl', 'wb') as f:
    pickle.dump(scaler, f)

with open('kmeans_model.pkl', 'wb') as f:
    pickle.dump(kmeans, f)

"""##**Overall Analysis and Insights:**

**1. Cluster Characteristics and Their Implications:**

Highly Developed Countries (Cluster 0):
* Strong economic indicators like high GDP per capita and robust health investments.
* Low child mortality and stable inflation reflect overall social and economic stability.
* Exports are significantly high, contributing to economic sustainability.
These countries represent the least need for external aid or interventions.

Moderately Developed Countries (Cluster 2):
* These countries show moderate GDP and child mortality rates, but they still face some challenges, particularly with inflation and health investment.
* Targeted aid and reforms could help stabilize these economies and improve their developmental trajectory.

Least Developed Countries (Cluster 1):
* Characterized by low GDP per capita, extremely high child mortality, and volatile inflation.
* Low exports and limited health investments reflect economic fragility and social challenges.
* These countries require urgent and substantial aid to address basic health, education, and infrastructure needs.

**2. Key Observations on Factors:**

**Exports:**
* Countries with high exports (especially in Cluster 0) have stronger economic stability. Policies promoting trade agreements and industrialization could benefit lower-exporting countries.

**Health Investments:**
* A direct correlation is observed between health investment and child mortality. Increasing health budgets could significantly reduce mortality rates in developing and least-developed countries.

**Inflation:**
* High inflation in clusters 1 and 2 points to economic instability. Financial policies aimed at stabilizing inflation can drive better economic outcomes.

**Child Mortality:**
* This is a key metric for assessing development levels. The major differences between clusters highlight where international aid and intervention should prioritize.

## **Recommendations:**


---


**For Policy Makers:**

**Focus on Basic Needs in Cluster 1 Countries:**

Increase health investments to address high child mortality rates.
Implement measures to stabilize inflation and encourage economic activity through microfinance and entrepreneurship programs.
Strengthen export-oriented sectors by providing incentives for industrial and agricultural productivity.



**Boost Economic Development in Cluster 2 Countries:**

Encourage foreign direct investment (FDI) and partnerships to build infrastructure and industries.
Introduce targeted healthcare initiatives to address moderate child mortality rates.
Focus on education and skill development to improve workforce productivity.



**Sustain Progress in Cluster 0 Countries:**

Promote innovation and research to maintain competitive advantages.
Strengthen resilience to potential economic shocks through diversification of exports and robust fiscal policies.

---

**For International Organizations and NGOs:**

**Aid Allocation:**

Prioritize Cluster 1 countries for financial aid and development projects, especially in healthcare and infrastructure.
Encourage global partnerships to mobilize resources for addressing extreme poverty and high mortality rates.

**Monitor Economic Indicators:**

Track GDP per capita, child mortality, and inflation trends to identify early signs of improvement or regression in targeted countries.

---

**Potential Developments:**

**Regional Collaboration:**

Encourage regional trade agreements and economic unions to strengthen exports in Clusters 1 and 2.
Collaborative health programs across neighboring countries can drive efficiency in resource allocation and service delivery.

**Innovative Financing:**

Introduce impact bonds or other innovative financing mechanisms to drive social outcomes in least-developed countries.

**Technology-Driven Solutions**:

Use data analytics and AI for efficient resource allocation and monitoring of aid effectiveness.
Promote digital education and e-health platforms to bridge gaps in education and healthcare services.

**Sustainability Initiatives:**

Focus on renewable energy investments in Clusters 1 and 2 to address energy needs and create jobs.
Encourage sustainable agricultural practices to improve exports and food security.

---

##**Conclusion:**
The analysis shows the importance of tailored interventions based on the development levels of different countries. While highly developed nations can sustain their progress independently, moderately and least-developed countries require targeted international support to address fundamental issues like health, inflation, and economic growth. By prioritizing these areas, we can bridge the gap in global development and promote a more equitable future.
"""
